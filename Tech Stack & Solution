# Proposal for Tech Stack & Solution  
## Personal Wardrobe Visual Similarity System

## System Architecture Overview

The proposed system is a machine learning–powered visual similarity platform designed to retrieve garments visually similar to a user-uploaded clothing image. The solution adopts a **dual-interface architecture**, consisting of a web-based application and a native iOS application, both connected to a unified Python backend. This design enables rapid prototyping while maintaining scalability and real-world deployment relevance.

---

## Frontend Layer

### Web Application
- **Framework:** Streamlit  
- **Purpose:** Rapid prototyping, model inspection, and interactive demonstration  
- **Functionality:**  
  - Clothing image upload  
  - Visualization of similarity scores and retrieved results  
  - Model output inspection for evaluation  

Streamlit is selected to support fast iteration and transparent demonstration of machine learning outputs, making it suitable for instructor review and experimentation.

### iOS Application
- **Language & Framework:** Swift with SwiftUI  
- **Purpose:** Mobile-first user interaction and real-world usage simulation  
- **Functionality:**  
  - Image capture and upload from device camera  
  - Display of visually similar clothing items  

The iOS application serves as a lightweight client that communicates with the backend API, demonstrating how the system can operate in a realistic consumer-facing environment.

---

## Backend & API Layer

- **Language:** Python  
- **Framework:** FastAPI  
- **Architecture:** RESTful, asynchronous API  

FastAPI is selected for its native support for asynchronous requests, which is critical for handling image uploads and machine learning inference. It also provides automatic API documentation (Swagger UI), facilitating development, debugging, and frontend integration.

**Core API endpoints include:**
- `POST /upload` – accepts an image and returns its visual embedding  
- `GET /similar` – retrieves visually similar items using vector similarity search  

---

## Machine Learning Stack

### Visual Embedding Model
- **Model:** CLIP (Contrastive Language–Image Pretraining)  
- **Purpose:** Extract high-level semantic image embeddings  

CLIP is chosen over traditional CNN-only models due to its ability to capture both visual and semantic relationships between garments, making it well-suited for fashion similarity tasks and extensible to future text-based search.

### Similarity Search
- **Method:** Cosine similarity in embedding space  
- **Storage:** Vector database (e.g., Faiss or ChromaDB)  

A vector database is used to efficiently store and query image embeddings, enabling fast similarity retrieval as the dataset grows. This approach is more scalable than traditional relational databases for embedding-based search.

---

## Data Storage

- **Metadata Storage:** SQLite or lightweight cloud database  
- **Embedding Storage:** Vector database (Faiss or ChromaDB)  
- **Image Storage:** Local file system or free-tier cloud storage  

Images are stored separately from metadata, with the database maintaining file paths and corresponding embedding vectors.

---

## Deployment Strategy

- **Web Application:** Local deployment or Streamlit Cloud  
- **Backend API:** Free-tier cloud hosting (e.g., Render or Fly.io)  
- **iOS Application:** Local testing or TestFlight distribution  
- **Inference:** Server-side model inference via FastAPI  

---

## Rationale & Alignment with Course Objectives

This tech stack:
- Demonstrates end-to-end deployment of a machine learning model  
- Supports a live, interactive web-based demo  
- Applies modern computer vision techniques to a real-world problem  
- Establishes a scalable foundation for future extensions  

By starting with visual similarity retrieval, the system minimizes implementation risk while delivering a technically robust and extensible proof-of-concept that satisfies the capstone requirement to design, build, and deploy a machine learning–powered application.
