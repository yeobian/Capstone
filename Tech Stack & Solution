# Proposal for Tech Stack & Solution (Updated After Professor Feedback)
## Personal Wardrobe Visual Similarity + Preference-Aware Retrieval (Web-First)

## System Architecture Overview
Based on instructor feedback, this project will prioritize a **web-based proof-of-concept** as the primary deliverable to ensure the system runs reliably as a live demo. The architecture is designed to be **extensible to iOS later**, but iOS development is treated as a future extension rather than a required deliverable for the capstone timeline.

The system is a machine learning–powered retrieval platform that:
1) accepts a clothing image (and optionally text),
2) generates embeddings using a shared model,
3) retrieves visually/semantically similar items,
4) adapts results using user preference signals (like/dislike).

---

## Frontend Layer (Primary Deliverable)

### Web Application
- **Framework:** Streamlit (or lightweight React later if needed)
- **Purpose:** Live demo + interactive user workflow
- **Functionality:**
  - Upload a clothing image
  - Return top-k similar results with similarity scores
  - Allow user feedback: **Like / Dislike**
  - Optional: text input (e.g., “more formal”, “no skinny jeans”, “summer vibe”)
  - Display “recommended different” results (diversity mode)

**Rationale:** Instructor emphasized focusing on a running web-based demo first to ensure feasibility and reliability.

---

## Backend & API Layer

- **Language:** Python
- **Framework:** FastAPI (preferred) or Flask
- **Architecture:** RESTful API

Core endpoints:
- `POST /embed` – accepts image (and optional text) and returns embeddings
- `GET /similar` – returns similar items using vector similarity search
- `POST /feedback` – stores user preference signals (like/dislike)
- `GET /recommend` – returns results adjusted by preference + diversity constraints

---

## Machine Learning Stack

### Shared Embedding Model (Image + Text)
- **Model:** CLIP (Contrastive Language–Image Pretraining)
- **Purpose:** Create embeddings in a shared space for both **image and language**
- **Why CLIP:** It supports similarity retrieval and allows future extension to language-based queries (e.g., “find something like this but more formal”).

### Similarity + Preference-Aware Retrieval
- **Baseline Retrieval:** cosine similarity in embedding space
- **Preference Adjustment (Web-First MVP):**
  - Store user feedback (like/dislike) for retrieved items
  - Learn a lightweight preference profile from embeddings of liked vs disliked items
  - Re-rank future retrieval results toward liked styles and away from disliked styles

### “Recommend Different” Feature (Instructor Suggestion)
- Add a diversity option:
  - if user says “I don’t like skinny jeans,” recommendations should avoid close neighbors in that style cluster
  - retrieve items that are still relevant but **intentionally farther** in embedding space

This supports realistic personalization without requiring large-scale user data.

---

## Vector Search & Storage

### Vector Similarity Search
- **Storage:** Faiss or ChromaDB (vector index for embeddings)
- **Method:** cosine similarity search for top-k retrieval

### Metadata and Images
- **Metadata Storage:** SQLite (items, tags, feedback logs)
- **Image Storage:** Local file storage (paths saved in DB) or free-tier cloud storage

Images are stored separately from metadata; the database stores file paths and embedding references.

---

## Deployment Strategy (Web-First)
- **Web App:** Local deployment or Streamlit Cloud
- **Backend API:** Free-tier hosting (Render or Fly.io)
- **Inference:** Server-side inference via API

---

## Future Extension (Not Required for Capstone)
### iOS Application (Optional Later)
- **SwiftUI** client consuming the same API endpoints
- This is deferred until the web demo is stable and complete.

---

## Rationale & Alignment with Course Objectives
This updated stack is intentionally scoped to ensure a successful live demo:
- End-to-end ML pipeline (upload → embed → retrieve → feedback → improved retrieval)
- Interactive web-based application
- Clear, realistic deliverables
- Extensible foundation for future mobile deployment

CLIP remains the core model choice because it supports both image and language embeddings, enabling translation between what users *show* (image) and what users *want* (text).
